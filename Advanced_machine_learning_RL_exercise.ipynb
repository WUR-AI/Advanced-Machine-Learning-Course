{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WUR-AI/Advanced-Machine-Learning-Course/blob/master/Advanced_machine_learning_RL_exercise.ipynb)\n"
      ],
      "metadata": {
        "id": "N3bTenJwURwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced machine learning - Reinforcement Learning Exercise"
      ],
      "metadata": {
        "id": "ohVxHCG9iDqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a DRL agent"
      ],
      "metadata": {
        "id": "Znrf9rDOio7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the exercises of reinforcement learning! In this exercise we will train two popular deep reinforcement learning agents that you have learned through your courses. This is the time to put that knowledge to practice!\n",
        "\n",
        "In the notebook, you will see a couple of ToDos. Try your best to work through them, and don't hesitate to ask for help!"
      ],
      "metadata": {
        "id": "3UONeAw_isM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import and install required libraries"
      ],
      "metadata": {
        "id": "rUaOfRoXjiym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q swig\n",
        "!pip install -q gymnasium[box2d]\n",
        "!pip install -q pyglet\n",
        "\n",
        "# install stable baselines that house the RL algorithms of DQN and PPO\n",
        "!pip install -q \"stable_baselines3[extra]\"\n",
        "\n",
        "# download a trained DQN\n",
        "!git clone https://github.com/WUR-AI/Advanced-Machine-Learning-Course.git"
      ],
      "metadata": {
        "id": "xHcN-6pE7BfZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below imports important libraries that will be used to train our RL agent. There are additional packages that will be used to visualize the RL agent in action. Since, google colab doesn't natively support visualizing the agent when calling render_mode=\"human\""
      ],
      "metadata": {
        "id": "c6zkIYLDD0cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import io\n",
        "from IPython.display import HTML\n",
        "import glob\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import typing\n",
        "import uuid\n",
        "from stable_baselines3 import DQN, PPO\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "#Set the seed for reproducibility\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "def visualize(model, env):\n",
        "    video_folder = f\"videos/{uuid.uuid4()}\"\n",
        "    env = RecordVideo(env, video_folder=video_folder, episode_trigger=lambda e: True)\n",
        "    obs, _ = env.reset(seed=10)\n",
        "    terminated, truncated = False, False\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "        if isinstance(model, DQN) or isinstance(model, PPO):\n",
        "            action, _state = model.predict(obs, deterministic=True)\n",
        "            action = int(action)  # In case it's a NumPy array\n",
        "        elif model == 'random':\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            raise ValueError(f\"Model {model} is not supported\")\n",
        "\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    mp4list = glob.glob(f'{video_folder}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        return HTML(data=f'''\n",
        "            <video width=\"640\" height=\"480\" controls>\n",
        "                <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n",
        "            </video>''')\n",
        "    else:\n",
        "        return \"No video found.\"\n"
      ],
      "metadata": {
        "id": "zW4AMoXHjFOI"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Gymnasium and the rocket landing problem\n",
        "\n",
        "In the following exercise we will train an agent to land a rocket on the moon. We will utilize the Gymnasium environment of the Lunar Lander. It is a problem of optimizing the thrusters of the rocket to land nicely on the pad, pulled by the moon's gravity. There are 3 thrusters available to use; the left, right and middle engine. The agent is rewarded on every timestep based on different factors: how far it is from the landing pad, the speed it's approaching the pad, the tilt angle of the rocket. It is also given a negative reward each time the engine is fired, discouraging it from using the thrusters too much.\n",
        "\n",
        "More information of the rocket landing environment is available [here](https://gymnasium.farama.org/environments/box2d/lunar_lander/)."
      ],
      "metadata": {
        "id": "cpjnazhmisUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the environment"
      ],
      "metadata": {
        "id": "H1t9zH6giscZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating an environment with the gymnasium package is relatively easy:"
      ],
      "metadata": {
        "id": "6pJ5cKCakjxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Wk9vVsZqh7AD",
        "outputId": "43d2e96c-8d0a-4c02-ce91-6166c96891ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "env_name = \"LunarLander-v3\"  #pre-made moon landing environment from gymnasium\n",
        "\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "\n",
        "#Set the seed\n",
        "env.action_space.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the properties of the environment"
      ],
      "metadata": {
        "id": "T3SMdNPhlvlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's always important to be familiar with an environment of an RL problem. Here, we look into the action space and observation space. Check [here](https://gymnasium.farama.org/introduction/basic_usage/#action-and-observation-spaces) for a description of the different spaces in the gymnasium."
      ],
      "metadata": {
        "id": "Qq4dKnAVwuqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The action space is {env.action_space}')\n",
        "print(f'The observation space is of {type(env.observation_space)} with shape {env.observation_space.shape} and contains {env.observation_space.dtype}')"
      ],
      "metadata": {
        "id": "gXRcs0xzv8kK",
        "outputId": "bb45bf2f-ec14-40d7-a61f-b4b7dbaae1e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The action space is Discrete(4)\n",
            "The observation space is of <class 'gymnasium.spaces.box.Box'> with shape (8,) and contains float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, the action space is discrete, with a continuous observation space.\n",
        "The action space consists of 4 discrete actions:\n",
        "\n",
        "\n",
        "*   0: do nothing\n",
        "*   1: fire left engine\n",
        "*   2: fire main engine\n",
        "*   3: fire right engine\n",
        "\n",
        "The observation space consists of an 8-dimensional vector, consisting of 6 continuous values and 2 booleans.\n",
        "\n",
        "To make the observation space a bit clearer, it's nice to put them into bins by extracting their max and min values.\n"
      ],
      "metadata": {
        "id": "IS1zaKtOxfR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low, high = env.observation_space.low, env.observation_space.high\n",
        "print(f'The lower values of the observation space is:\\n{low}\\n\\n and the upper values are \\n{high}')"
      ],
      "metadata": {
        "id": "33DsZ6Xqz9lY",
        "outputId": "2078ab60-29e7-422f-a940-d4d2a78a041b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lower values of the observation space is:\n",
            "[ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ]\n",
            "\n",
            " and the upper values are \n",
            "[ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see the value ranges of the observation space. The x and y coordinate ranges are $[-2.5, 2.5]$ (the landing pad is at $(0,0)$), the linear velocity ranges (in x and y) are $[-10, 10]$, the angle is in $[-2\\pi, 2\\pi]$ and the angular velocity is in $[-10, 10]$. The last two are booleans that represent the contact of the legs with the ground when landing."
      ],
      "metadata": {
        "id": "TEM5pqnamC20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can reset the environment to the start of an episode with this line of code:"
      ],
      "metadata": {
        "id": "zbl6ZG51mC5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "0LaRLrGK8ErJ",
        "outputId": "56fc6fc4-bbb6-4092-db19-22e02431c4cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-3.4141540e-05,  1.4010243e+00, -3.4739270e-03, -4.3981442e-01,\n",
              "         4.6357098e-05,  7.8687706e-04,  0.0000000e+00,  0.0000000e+00],\n",
              "       dtype=float32),\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can see the state of the environment at the start of an episode. Next, we can sample some of the actions of the agent with the following line of code. Try running it a few times to see what the agent does."
      ],
      "metadata": {
        "id": "dnAEebd7DPgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.sample()"
      ],
      "metadata": {
        "id": "3kNEn7QrFo82",
        "outputId": "658a3449-1512-433e-cbd3-82938be4de16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TODO 1:\n",
        "\n",
        "\n",
        "*   When sampling the environment actions, what does it mean when it shows the number 3?\n"
      ],
      "metadata": {
        "id": "9rxcM5mbFtgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write the TODO here"
      ],
      "metadata": {
        "id": "ITIyeDgXFSRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seeing a random agent in action\n",
        "\n",
        "To get even more familiarized with the environment, we will see our agent on screen. With the function visualize() (defined in the 2nd cell) we can see our agent in action. We will call it with 'random'; meaning it will do random actions. Click on the \"start button\" in the video to see your agent in action."
      ],
      "metadata": {
        "id": "s1D9bWimIGZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize('random', env)"
      ],
      "metadata": {
        "id": "Z6eXd0Ir2Z8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will most likely have seen the agent fail miserably to land the rocket, or just fly out of screen - never to be seen again. Hence, we need an agent with some intelligence to land the rocket. Here, we will move on to train the agent with two fundamental RL algorithms; DQN and PPO."
      ],
      "metadata": {
        "id": "PUV_RfrsYs9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To conclude:\n",
        "\n",
        "The important functions for the environment are as follows:\n",
        "- **env.reset():**\n",
        "    Resets the environment and obtain initial starting observation\n",
        "- **env.step(action):**\n",
        "    Applies an action to it. It outputes next state, reward, terminate, truncate, and info"
      ],
      "metadata": {
        "id": "MaoxOWcL8D0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a Q-learning agent\n",
        "\n",
        "Let's go into the meat of the problem: training an agent with a deep Q learning method.\n",
        "In this exercise, we will use the stable_baselines3 implementation of the DQN algorithm.\n",
        "\n",
        "The theory behind the DQN algorithm you have learned in class. In essence, the idea behind Q-learning is that if we had a function\n",
        "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
        "us what our return would be, if we were to take an action in a given\n",
        "state, then we could easily construct a policy that maximizes our\n",
        "rewards:\n",
        "\n",
        "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
        "\n",
        "For our training update rule, we'll use a fact that every $Q$\n",
        "function for some policy obeys the Bellman equation:\n",
        "\n",
        "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
        "\n",
        "OK, fun equations, right? To move forward, we will try to train a DQN agent with 10.000 steps."
      ],
      "metadata": {
        "id": "E0PA7xFzNxxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN, PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Train a basic DQN agent without changing its hyperparameters\n",
        "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10_000)"
      ],
      "metadata": {
        "id": "LQR8UFOMN5Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, we can try evaluating (testing) the policy with the line of code below. Testing in RL means we will plop a learned agent in its environment and let it run while we record the rewards it obtains. We will test with 10 episodes:"
      ],
      "metadata": {
        "id": "rKov5WRqc9C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
      ],
      "metadata": {
        "id": "apByJWKDcZxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is the average reward? Nevertheless, after training the agent for around 100 episodes, we can try and see it in action:"
      ],
      "metadata": {
        "id": "8vwF0S3PZnlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(model, env)"
      ],
      "metadata": {
        "id": "v0uKJ-oMSXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did the agent do? Most likely not so good. We can try and tweak the training hyper-parameters. We can try tweaking the number of steps, the exploration rate and the policy networks of the RL agent.\n",
        "\n",
        "\n",
        "Note: the number of steps here mean each time a step is taken (env.step(action)), to distinguish it from number of episodes.\n",
        "\n",
        "#### TODO 2:\n",
        "Our trained DQN consists of two neural networks: a Q network and a target Q network. Given that we have an observation space of size 8, and an action space of size 4, what would be the number of inputs in the first layer of the networks? You can check your answer by printing the networks as shown below"
      ],
      "metadata": {
        "id": "1UT-c361aQ6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(model.policy)"
      ],
      "metadata": {
        "id": "6j7aLaW-0B_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've understood the inputs and outputs, let's move on to the next task!\n",
        "\n",
        "* Fill in below three hyperparameters: (1) number of steps of training, (2) the fraction of the whole training that the agent will be in \"explore\" mode - i.e. doing random actions, and (3) the final random action probabililty, that you deem would let the agent find a good policy.\n",
        "Justify your choices!"
      ],
      "metadata": {
        "collapsed": false,
        "id": "OxnUe81i0B_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters to tweak\n",
        "\n",
        "n_steps =\n",
        "hyperparams_dqn = {'exploration_fraction': ,\n",
        "'exploration_final_eps':\n",
        "}"
      ],
      "metadata": {
        "id": "FBZ1xV0ngymc"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for checking the agent's performance\n",
        "import os\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "\n",
        "# function take from https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
        "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
        "\n",
        "    :param check_freq:\n",
        "    :param log_dir: Path to the folder where the model will be saved.\n",
        "      It must contains the file created by the ``Monitor`` wrapper.\n",
        "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.log_dir = log_dir\n",
        "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "\n",
        "            # Retrieve training reward\n",
        "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
        "            if len(x) > 0:\n",
        "                # Mean training reward over the last 100 episodes\n",
        "                mean_reward = np.mean(y[-100:])\n",
        "                if self.verbose >= 1:\n",
        "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
        "                    print(\n",
        "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
        "\n",
        "                # New best model, you could save the agent here\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    # Example for saving best model\n",
        "                    if self.verbose >= 1:\n",
        "                        print(f\"Saving new best model to {self.save_path}\")\n",
        "                    self.model.save(self.save_path)\n",
        "\n",
        "        return True\n",
        "\n",
        "\n",
        "# Create log dir\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create the callback: check every 1000 steps\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n"
      ],
      "metadata": {
        "id": "04e6tIqqGSx7"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent with your tweaked hyperparameters\n",
        "# Add logging to check the agent's performance during training\n",
        "\n",
        "# Make the evironment\n",
        "env_name = \"LunarLander-v3\"\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "env = Monitor(env, log_dir)\n",
        "\n",
        "#seed for reproducability\n",
        "seed = 5\n",
        "\n",
        "# Train a the DQN agent\n",
        "model = DQN(\"MlpPolicy\", env, seed=seed, verbose=1, **hyperparams_dqn)\n",
        "model.learn(total_timesteps=n_steps, callback=callback)"
      ],
      "metadata": {
        "id": "XjO1nVXKa6Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
      ],
      "metadata": {
        "id": "7GCWS6qgjiVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_eps, eps = evaluate_policy(model, model.get_env(), n_eval_episodes=10, return_episode_rewards=1)\n",
        "print(f\"The reward per episode is {reward_eps} and the length of each episode is {eps}\")"
      ],
      "metadata": {
        "id": "yTPs3xQGh1ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results([log_dir], n_steps, results_plotter.X_TIMESTEPS, \"DQN LunarLander\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZPLALXVNHK7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TO DO 3:\n",
        "\n",
        "* how did your DQN agent do?\n",
        "* Did you think the hyperparameters you chose were good enough? Do you think there are [additional paramaters](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#stable_baselines3.dqn.DQN) that are worthwhile to tweak? Describe what you think.\n",
        "* What do you think is the most important parameter for the environment of the moon lander?"
      ],
      "metadata": {
        "id": "eMfGPTIQiuz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can continue to train and tweak the agent on your own.\n",
        "In the following section, you can load a trained agent with optimized parameters."
      ],
      "metadata": {
        "id": "Z7Av8Cgu4DcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the evironment\n",
        "env_name = \"LunarLander-v3\"\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "model = DQN.load(\"/content/Advanced-Machine-Learning-Course/dqn-LunarLander.zip\")"
      ],
      "metadata": {
        "id": "OH40uPrQlG8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
      ],
      "metadata": {
        "id": "KLrDL0ACWKFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the loaded model"
      ],
      "metadata": {
        "id": "RA1j5dyYFft8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(model, env)"
      ],
      "metadata": {
        "id": "7R3aAfGbnwjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the learned DQN agent is able to land the rocket on the moon properly."
      ],
      "metadata": {
        "id": "0p_7oFSuIYN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Policy Gradient agent"
      ],
      "metadata": {
        "id": "MhvUu34hQZPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About PPO\n",
        "\n",
        "You might have had a difficult time training the DQN agent due to its sensitivity to hyperparameters. This is partly because DQN estimates action values (Q-values), and in some environments, it tends to overestimate the expected rewards for certain actions. This overestimation can lead to unstable training and divergence from the optimal policy.\n",
        "\n",
        "**Proximal Policy Optimization (PPO)** addresses this issue by **clipping** the policy update. Instead of allowing large, potentially destabilizing updates to the policy, PPO restricts how much the new policy is allowed to deviate from the old one during each update step.\n",
        "\n",
        "This is done using the following clipped objective function:\n",
        "\n",
        "$$\n",
        "L^{\\text{CLIP}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$ is the probability ratio between the new and old policies.  \n",
        "- $\\hat{A}_t$ is the estimated advantage at time $t$.  \n",
        "- $\\epsilon$ is a small hyperparameter (commonly 0.1 or 0.2) that determines how much the new policy can deviate from the old one.\n",
        "\n",
        "This clipping mechanism helps ensure **more stable and conservative updates**, avoiding the kind of over-optimistic updates that can occur in DQN.\n",
        "\n",
        "---\n",
        "\n",
        "Now that we've covered the theory, let's train the agent using the `stable-baselines3` implementation of PPO. We'll start with 10,000 training steps.\n"
      ],
      "metadata": {
        "id": "IWV1eZajPDa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Make the evironment\n",
        "env_name = \"LunarLander-v3\"\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "\n",
        "seed = 0\n",
        "\n",
        "# Train a the DQN agent\n",
        "model = PPO(\"MlpPolicy\", env, gamma=0.9, seed=seed, verbose=1)\n",
        "model.learn(total_timesteps=10_000)"
      ],
      "metadata": {
        "id": "3hnqnCAzQY1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
      ],
      "metadata": {
        "id": "-UPf1-B3oYk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training 10.000 steps might not be enough for the PPO agent.\n",
        "\n",
        "#### TO DO 4:\n",
        "* Tweak the [hyperparameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO) of the PPO agent. Which parameters do you think are the most suitable for training the moon lander?\n",
        "* Train a PPO agent, subsequently evaluate the agent and explain the changes you made."
      ],
      "metadata": {
        "id": "z-OfcJ6mDAf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fill in values of the hyperparameters below\n",
        "\n",
        "n_steps =\n",
        "hyperparams_ppo = {'batch_size':,\n",
        "'n_steps': ,\n",
        "'gamma':,\n",
        "'clip_range':\n",
        "}"
      ],
      "metadata": {
        "id": "sfPtvgBHo8A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create log dir\n",
        "log_dir = \"tmp/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Create the callback: check every 1000 steps\n",
        "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
        "\n",
        "# Make the evironment\n",
        "env_name = \"LunarLander-v3\"\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "env = Monitor(env, log_dir)\n",
        "\n",
        "seed = 5\n",
        "\n",
        "# Train a the PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, seed=seed, verbose=1, **hyperparams_ppo)\n",
        "model.learn(total_timesteps=n_steps, callback=callback)"
      ],
      "metadata": {
        "id": "K45IEViepzi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
      ],
      "metadata": {
        "id": "tDXo1zYLrkdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results([log_dir], n_steps, results_plotter.X_TIMESTEPS, \"PPO LunarLander\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xj-VbBVkJrkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(model, env)"
      ],
      "metadata": {
        "id": "kC2QqAghrlI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your best model will be saved in the files section of google colab. This file you can download and run again; or even continue with the training.\n",
        "\n",
        "Here you have learned to train an RL agent to land a rocket on the moon with two popular RL algorithms. There are some differences between the two algorithms."
      ],
      "metadata": {
        "id": "2Qd7Xi2simXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TO DO 5:\n",
        "*   What are the main differences between Q-learning and Gradient Policy algorithm?\n",
        "* What does it mean that DQN learns off-policy and PPO learns on-policy?\n",
        "* What are your thoughts about when to use either DQN or PPO?"
      ],
      "metadata": {
        "id": "0kd-pzDEZRvW"
      }
    }
  ]
}
