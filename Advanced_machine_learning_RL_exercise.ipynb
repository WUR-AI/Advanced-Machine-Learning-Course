{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced machine learning - Reinforcement Learning Exercise"
   ],
   "metadata": {
    "id": "ohVxHCG9iDqz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training a DRL agent"
   ],
   "metadata": {
    "id": "Znrf9rDOio7q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Welcome to the exercises of reinforcement learning! In this exercise we will train two popular deep reinforcement learning agents that you have learned through your courses. This is the time to put that knowledge to practice!\n",
    "\n",
    "In the notebook, you will see a couple of ToDos. Try your best to work through them, and don't hesitate to ask for help!"
   ],
   "metadata": {
    "id": "3UONeAw_isM6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Import and install required libraries"
   ],
   "metadata": {
    "id": "rUaOfRoXjiym"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# install required system dependencies for visualization\n",
    "!apt-get install -y xvfb x11-utils\n",
    "\n",
    "# install required python dependencies\n",
    "!pip install -q swig\n",
    "!pip install -q gymnasium[box2d] == 0.28.1\n",
    "\n",
    "# install for the visualization of the agent\n",
    "!pip install -q pyvirtualdisplay == 0.2.* PyOpenGL == 3.1.* PyOpenGL-accelerate == 3.1.*\n",
    "\n",
    "# install stable baselines that house the RL algorithms of DQN and PPO\n",
    "!pip install \"stable_baselines3[extra]>=2.0.0a9\"\n",
    "\n",
    "# download a trained DQN\n",
    "!git clone https: // github.com/BigDataWUR/Advanced-Machine-Learning-Course.git"
   ],
   "metadata": {
    "id": "xHcN-6pE7BfZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cell below imports important libraries that will be used to train our RL agent. There are additional packages that will be used to visualize the RL agent in action. Since, google colab doesn't natively support visualizing the agent when calling render_mode=\"human\""
   ],
   "metadata": {
    "id": "c6zkIYLDD0cn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import typing\n",
    "from stable_baselines3 import DQN, PPO\n",
    "\n",
    "#Set the seed for reproducibility\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "#create the 'virtual' screen\n",
    "Display(visible=0, size=(400, 300)).start()\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "# function for visualizing the agent\n",
    "def visualize(model, env):\n",
    "    img = plt.imshow(env.render())\n",
    "    terminated, truncated = False, False\n",
    "    if isinstance(model, DQN) or isinstance(model, PPO):\n",
    "        vec_env = model.get_env()\n",
    "        obs = vec_env.reset()\n",
    "    while not terminated or truncated:\n",
    "        if isinstance(model, DQN) or isinstance(model, PPO):\n",
    "            action, _state = model.predict(obs, deterministic=True)\n",
    "            action = int(action)\n",
    "        else:\n",
    "            action = model\n",
    "        img.set_data(env.render())\n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.close()"
   ],
   "metadata": {
    "id": "zW4AMoXHjFOI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Gymnasium and the rocket landing problem\n",
    "\n",
    "In the following exercise we will train an agent to land a rocket on the moon. We will utilize the OpenAI gym 2D environment of the Lunar Lander. It is a problem of optimizing the thrusters of the rocket to land nicely on the pad, pulled by the moon's gravity. There are 3 thrusters available to use; the left, right and middle engine. The agent is rewarded on every timestep based on different factors: how far it is from the landing pad, the speed it's approaching the pad, the tilt angle of the rocket. It is also given a negative reward each time the engine is fired, discouraging it from using the thrusters too much.\n",
    "\n",
    "More information of the rocket landing environment is available [here](https://gymnasium.farama.org/environments/box2d/lunar_lander/)."
   ],
   "metadata": {
    "id": "cpjnazhmisUp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the environment"
   ],
   "metadata": {
    "id": "H1t9zH6giscZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating an environment with the gymnasium package is relatively easy:"
   ],
   "metadata": {
    "id": "6pJ5cKCakjxL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk9vVsZqh7AD"
   },
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"  #pre-made moon landing environment from gymnasium\n",
    "\n",
    "# the render mode is for showing the agent in each step. Change to \"human\" when running on your local machine.\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "#Set the seed\n",
    "env.action_space.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the properties of the environment"
   ],
   "metadata": {
    "id": "T3SMdNPhlvlM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's always important to be familiar with an environment of an RL problem. Here, we look into the action space and observation space. Check [here](hthttps://gymnasium.farama.org/content/basic_usage/#action-and-observation-spacestps://) for a description of the different spaces in the gymnasium."
   ],
   "metadata": {
    "id": "Qq4dKnAVwuqi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'The action space is {env.action_space}')\n",
    "\n",
    "print(f'The observation space is {env.observation_space}')"
   ],
   "metadata": {
    "id": "gXRcs0xzv8kK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see above, the action space is discrete, with a continuous observation space.\n",
    "The action space consists of 4 discrete actions:\n",
    "\n",
    "\n",
    "*   0: do nothing\n",
    "*   1: fire left engine\n",
    "*   2: fire main engine\n",
    "*   3: fire right engine\n",
    "\n",
    "The observation space consists of an 8-dimensional vector, consisting of 6 continuous values and 2 booleans.\n",
    "\n",
    "To make the observation space a bit clearer, it's nice to put them into bins by extracting their max and min values.\n"
   ],
   "metadata": {
    "id": "IS1zaKtOxfR3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "low, high = env.observation_space.low, env.observation_space.high\n",
    "print(f'The lower values of the observation space is:\\n{low}\\n\\n and the upper values are \\n{high}')"
   ],
   "metadata": {
    "id": "33DsZ6Xqz9lY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we can see the value ranges of the observation space. The x and y coordinate ranges are $[-90, 90]$, the angle ranges are $[-5, 5]$ and the angular velocity values are $[-3.14, 3.14]$. The last two are booleans that represent the contact of the legs with the ground when landing."
   ],
   "metadata": {
    "id": "TEM5pqnamC20"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can reset the environment to the start of an episode with this line of code:"
   ],
   "metadata": {
    "id": "zbl6ZG51mC5u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env.reset()"
   ],
   "metadata": {
    "id": "0LaRLrGK8ErJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can see the state of the environment at the start of an episode. Next, we can sample some of the actions of the agent with the following line of code. Try running it a few times to see what the agent does."
   ],
   "metadata": {
    "id": "dnAEebd7DPgJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env.action_space.sample()"
   ],
   "metadata": {
    "id": "3kNEn7QrFo82"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### TODO 1:\n",
    "\n",
    "\n",
    "*   When sampling the environment actions, what does it mean when it shows the number 3?\n",
    "*   Write below the line of code that can be used to check the shape of the observation space\n",
    "\n"
   ],
   "metadata": {
    "id": "9rxcM5mbFtgv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#write the TODO here"
   ],
   "metadata": {
    "id": "ITIyeDgXFSRG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Seeing a random agent in action\n",
    "\n",
    "To get even more familiarized with the environment, we will see our agent on screen. With the function visualize() (defined in the 2nd cell) we can see our agent in action. We will call it with the sample function; meaning it will do random actions.\n",
    "\n",
    "*To note, a google colab notebook does not natively support rendering the 2D environments from gymnasium. So we made a hacky workaround to still be able to show it with matplotlib, hence is why the render is a bit laggy."
   ],
   "metadata": {
    "id": "s1D9bWimIGZ5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "visualize(env.action_space.sample(), env)"
   ],
   "metadata": {
    "id": "jTVCOhsWAem6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will most likely have seen the agent fail miserably to land the rocket, or just fly out of screen - never to be seen again. Hence, we need an agent with some intelligence to land the rocket. Here, we will move on to train the agent with two fundamental RL algorithms; DQN and PPO."
   ],
   "metadata": {
    "id": "PUV_RfrsYs9N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## To conclude:\n",
    "\n",
    "The important functions for the environment are as follows:\n",
    "- **env.reset():** \n",
    "    Resets the environment and obtain initial starting observation\n",
    "- **env.step(action):** \n",
    "    Applies an action to it. It outputes next state, reward, terminate, truncate, and info"
   ],
   "metadata": {
    "id": "MaoxOWcL8D0G"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training a Q-learning agent\n",
    "\n",
    "Let's go into the meat of the problem: training an agent with a deep Q learning method.\n",
    "In this exercise, we will use the stable_baselines3 implementation of the DQN algorithm.\n",
    "\n",
    "The theory behind the DQN algorithm you have learned in class. In essence, the idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
    "us what our return would be, if we were to take an action in a given\n",
    "state, then we could easily construct a policy that maximizes our\n",
    "rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "OK, fun equations, right? To move forward, we will try to train a DQN agent with 10.000 steps."
   ],
   "metadata": {
    "id": "E0PA7xFzNxxC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Train a basic DQN agent without changing its hyperparameters\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)"
   ],
   "metadata": {
    "id": "LQR8UFOMN5Ef"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "dF4pvkgFsQBc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After training, we can try evaluating (testing) the policy with the line of code below. Testing in RL means we will plop a learned agent in its environment and let it run while we record the rewards it obtains. We will test with 10 episodes:"
   ],
   "metadata": {
    "id": "rKov5WRqc9C6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
   ],
   "metadata": {
    "id": "apByJWKDcZxr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "How is the average reward? Nevertheless, after training the agent for around 100 episodes, we can try and see it in action:"
   ],
   "metadata": {
    "id": "8vwF0S3PZnlj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "visualize(model, env)"
   ],
   "metadata": {
    "id": "v0uKJ-oMSXod"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "How did the agent do? Most likely not so good. We can try and tweak the training hyper-parameters. We can try tweaking the number of steps, the exploration rate and the policy networks of the RL agent.\n",
    "\n",
    "\n",
    "Note: the number of steps here mean each time a step is taken (env.step(action)), to distinguish it from number of episodes.\n",
    "\n",
    "#### TODO 2:\n",
    "Print the model's policy in the code below and examine the neural network shape. Note that it has two neural networks a Q network and a target Q network."
   ],
   "metadata": {
    "id": "1UT-c361aQ6U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The Qnetwork has a size of 8 as an input. What are this 8 inputs?\n",
    "- Moreover, the Qnetwork has a size of 4 as outputs. What are these 4 outputs?\n",
    "\n",
    "Hint: check this [link](https://gymnasium.farama.org/environments/box2d/lunar_lander/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once you've understood the inputs and outputs, let's move on to the next task!\n",
    "\n",
    "* Fill in below three hyperparameters: (1) number of steps of training, (2) the fraction of the whole training that the agent will be in \"explore\" mode - i.e. doing random actions, and (3) the final random action probabililty, that you deem would let the agent find a good policy.\n",
    "Justify your choices!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#hyperparameters to tweak\n",
    "\n",
    "n_steps =\n",
    "hyperparams_dqn = {'exploration_fraction':,\n",
    "'exploration_final_eps':\n",
    "}"
   ],
   "metadata": {
    "id": "FBZ1xV0ngymc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#Code for checking the agent's performance\n",
    "import os\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "# function take from https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose >= 1:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose >= 1:\n",
    "                        print(f\"Saving new best model to {self.save_path}\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n"
   ],
   "metadata": {
    "id": "04e6tIqqGSx7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the agent with your tweaked hyperparameters\n",
    "# Add logging to check the agent's performance during training\n",
    "\n",
    "# Make the evironment\n",
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "#seed for reproducability\n",
    "seed = 5\n",
    "\n",
    "# Train a the DQN agent\n",
    "model = DQN(\"MlpPolicy\", env, seed=seed, verbose=1, **hyperparams_dqn)\n",
    "model.learn(total_timesteps=n_steps, callback=callback)"
   ],
   "metadata": {
    "id": "XjO1nVXKa6Wa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
   ],
   "metadata": {
    "id": "7GCWS6qgjiVS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reward_eps, eps = evaluate_policy(model, model.get_env(), n_eval_episodes=10, return_episode_rewards=1)\n",
    "print(f\"The reward per episode is {reward_eps} and the length of each episode is {eps}\")"
   ],
   "metadata": {
    "id": "yTPs3xQGh1ZF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_results([log_dir], n_steps, results_plotter.X_TIMESTEPS, \"DQN LunarLander\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ZPLALXVNHK7N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TO DO 3:\n",
    "\n",
    "* how did your DQN agent do?\n",
    "* Did you think the hyperparameters you chose were good enough? Do you think there are [additional paramaters](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#stable_baselines3.dqn.DQN) that are worthwhile to tweak? Describe what you think.\n",
    "* What do you think is the most important parameter for the environment of the moon lander?"
   ],
   "metadata": {
    "id": "eMfGPTIQiuz7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can continue to train and tweak the agent on your own.\n",
    "In the following section, you can load a trained agent with optimized parameters."
   ],
   "metadata": {
    "id": "Z7Av8Cgu4DcM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Make the evironment\n",
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "model = DQN.load(\"/content/Advanced-Machine-Learning-Course/dqn-LunarLander\", env=env)"
   ],
   "metadata": {
    "id": "OH40uPrQlG8M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
   ],
   "metadata": {
    "id": "KLrDL0ACWKFa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the loaded model"
   ],
   "metadata": {
    "id": "RA1j5dyYFft8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "visualize(model, env)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "7R3aAfGbnwjk",
    "outputId": "ec21fe1d-c518-43c1-d78e-4c722cead023"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see the learned DQN agent is able to land the rocket on the moon properly. This is done with a lot of effort optimizing the hyperparameters."
   ],
   "metadata": {
    "id": "0p_7oFSuIYN-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a Policy Gradient agent"
   ],
   "metadata": {
    "id": "MhvUu34hQZPO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will train an agent with a policy gradient algorithm. A very popular and tested RL algorithm is Proximal Policy Optimization or PPO.\n",
    "\n",
    "### about PPO\n",
    "\n",
    "You might have had a difficult time training the DQN agent with the various available hyperparameters. This is due to the nature of DQN and how it estimates the action values to the rewards (as shown in the equations a bit earlier in the notebook). In some environment, DQN tends to overestimate the reward value of a specific (discrete) action, which then overshoots and leaves the training off-track from the actual optimal policy. PPO tries to mitigate this disadvantage by clipping the action value updates so the agent doesn't overshoot its learning to a specific action.\n",
    "\n",
    "How it does the clipping is given by the following function:\n",
    "\n",
    "\\begin{align}L^{CLIP}(\\theta) = \\hat{\\mathbb{E_t}}\\left[\\min(\n",
    "    r_t(\\theta)\\hat{A_t},clip(r_t(\\theta),1-\\in,1+\\in)\\hat{A_t})\n",
    "    )\\right]\\end{align}\n",
    "\n",
    "In essence, this function samples a batch of data (amount can be tweaked) experienced in the policy network of the environment for a specific number of steps (also can be tweaked), and subsequently clips the policy update to a certain amount (again, also can be tweaked) to lower the return estimates of the action value. This way, a theoretically more stable learning process can be achieved.\n",
    "\n",
    "Enough of theory; let's move on to the training. We will use the stable_baslines3 implementation of PPO to train the agent. Let's try training with 10.000 steps.\n"
   ],
   "metadata": {
    "id": "G2S5IvNT5lR7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Make the evironment\n",
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "\n",
    "seed = 0\n",
    "\n",
    "# Train a the DQN agent\n",
    "model = PPO(\"MlpPolicy\", env, gamma=0.9, seed=seed, verbose=1)\n",
    "model.learn(total_timesteps=10_000)"
   ],
   "metadata": {
    "id": "3hnqnCAzQY1x"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
   ],
   "metadata": {
    "id": "-UPf1-B3oYk2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training 10.000 steps might not be enough for the PPO agent.\n",
    "\n",
    "#### TO DO 4:\n",
    "* Tweak the [hyperparameters](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO) of the PPO agent. Which parameters do you think are the most suitable for training the moon lander?\n",
    "* Train a PPO agent, subsequently evaluate the agent and explain the changes you made."
   ],
   "metadata": {
    "id": "z-OfcJ6mDAf8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#fill in values of the hyperparameters below\n",
    "\n",
    "n_steps =\n",
    "hyperparams_ppo = {'batch_size':,\n",
    "'n_steps': ,\n",
    "'gamma':,\n",
    "'clip_range':\n",
    "}"
   ],
   "metadata": {
    "id": "sfPtvgBHo8A_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create log dir\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir)\n",
    "\n",
    "# Make the evironment\n",
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "seed = 5\n",
    "\n",
    "# Train a the PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, seed=seed, verbose=1, **hyperparams_ppo)\n",
    "model.learn(total_timesteps=n_steps, callback=callback)"
   ],
   "metadata": {
    "id": "K45IEViepzi1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"The mean reward is {mean_reward} and the standard deviation of the reward is {std_reward}\")"
   ],
   "metadata": {
    "id": "tDXo1zYLrkdc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_results([log_dir], n_steps, results_plotter.X_TIMESTEPS, \"PPO LunarLander\")\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Xj-VbBVkJrkO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "visualize(model, env)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "kC2QqAghrlI9",
    "outputId": "424b2de8-b20c-4968-dffb-82b96f622575"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your best model will be saved in the files section of google colab. This file you can download and run again; or even continue with the training.\n",
    "\n",
    "Here you have learned to train an RL agent to land a rocket on the moon with two popular RL algorithms. There are some differences between the two algorithms."
   ],
   "metadata": {
    "id": "2Qd7Xi2simXZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TO DO 5:\n",
    "*   What are the main differences between Q-learning and Gradient Policy algorithm?\n",
    "* What does it mean that DQN learns off-policy and PPO learns on-policy?\n",
    "* What are your thoughts about when to use either DQN or PPO?"
   ],
   "metadata": {
    "id": "0kd-pzDEZRvW"
   }
  }
 ]
}
